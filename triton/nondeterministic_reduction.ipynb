{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 非确定性运算---规约运算\n",
    "\n",
    "Author: kaiyuan\n",
    "\n",
    "Email: kyxie@zju.edu.cn\n",
    "\n"
   ],
   "id": "5f4362b2a249ffe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1 浮点数的结合律\n",
    "\n",
    "在计算机系统中，浮点数运算并不严格遵循加法结合律， 可能出现：\n",
    "\n",
    "(a + b) + c  != a + (b + c)\n",
    "\n",
    "示例：\n"
   ],
   "id": "e5aecf4f3967969e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# (a + b) + c  != a + (b + c) 浮点数计算示例：\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# float32能精确表示的最大连续整数：2^24=16777216\n",
    "a = np.float32(16777216.0)\n",
    "b = np.float32(1.0)\n",
    "c = np.float32(-16777216.0)\n",
    "\n",
    "# 强制存储结果，避免硬件扩展精度干扰\n",
    "left = np.float32((a + b) + c)\n",
    "right = np.float32(a + (b + c))\n",
    "\n",
    "print(\"(a+b)+c =\", left)   # 输出 0.0\n",
    "print(\"a+(b+c) =\", right)  # 输出 1.0\n",
    "print(\"是否相等:\", left == right)  # 输出 False"
   ],
   "id": "3f2cc0ebd902bf70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2 Triton中线程块对规约运算的影响\n",
    "思路：通过Trition来演示非确定性运算，调整线程块BLOCK_SIZE来对比计算差异。\n",
    "\n",
    "* 算子：规约(求和)运算。\n",
    "* 参考系：pytorch自带的加法运算\n",
    "* 数据类型：FP16、FP3\n",
    "\n",
    "依赖包 torch、triton，建议在镜像环境下运行。\n",
    "\n",
    "推荐：nvcr.io/nvidia/pytorch\n",
    "\n",
    "## 2.1 运行代码\n"
   ],
   "id": "7c2a0f7254e0cfe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def reduction_kernel_simple(\n",
    "        x_ptr,  # 输入指针\n",
    "        output_ptr,  # 输出指针\n",
    "        n_elements,  # 元素数量\n",
    "        BLOCK_SIZE: tl.constexpr,  # 线程块大小\n",
    "):\n",
    "    \"\"\"简单的归约核函数\"\"\"\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    # 加载数据\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n",
    "\n",
    "    # 简单的归约\n",
    "    accumulator = tl.sum(x)\n",
    "\n",
    "    # 存储结果\n",
    "    tl.store(output_ptr + pid, accumulator)\n",
    "\n",
    "\n",
    "def demonstrate_nondeterministic_triton():\n",
    "    \"\"\"演示Triton的非确定性计算\"\"\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Triton非确定性计算演示\")\n",
    "    print(\"使用不同线程块大小和内存访问模式\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 检查设备\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"警告: 未检测到CUDA设备\")\n",
    "        return\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"使用设备: {device}\")\n",
    "\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "\n",
    "    # 测试1: 不同线程块大小的向量归约\n",
    "    print(\"\\n1. 测试不同线程块大小的向量归约\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # 创建数据\n",
    "    vec_size = 2 ** 20  # 1M元素\n",
    "    vector_fp16 = torch.randn(vec_size, dtype=torch.float16, device=device)\n",
    "    vector_fp32 = vector_fp16.float()\n",
    "\n",
    "    print(f\"向量大小: {vec_size:,}\")\n",
    "    print(f\"数据范围: [{vector_fp16.min().item():.3f}, {vector_fp16.max().item():.3f}]\")\n",
    "\n",
    "    # PyTorch基准\n",
    "    torch_sum_fp16 = vector_fp16.sum().item()\n",
    "    torch_sum_fp32 = vector_fp32.sum().item()\n",
    "    print(f\"\\nPyTorch sum (float16): {torch_sum_fp16:.10f}\")\n",
    "    print(f\"PyTorch sum (float32): {torch_sum_fp32:.10f}\")\n",
    "    print(f\"两者差异: {abs(torch_sum_fp16 - torch_sum_fp32):.10f}\")\n",
    "\n",
    "    # 不同线程块大小\n",
    "    block_sizes = [64, 128, 256, 512, 1024]\n",
    "    results_fp16 = []\n",
    "    results_fp32 = []\n",
    "\n",
    "    for block_size in block_sizes:\n",
    "        # 计算网格大小\n",
    "        num_blocks = (vec_size + block_size - 1) // block_size\n",
    "        grid = (num_blocks,)\n",
    "\n",
    "        # 创建输出张量\n",
    "        output_fp16 = torch.zeros(num_blocks, dtype=torch.float32, device=device)\n",
    "        output_fp32 = torch.zeros(num_blocks, dtype=torch.float32, device=device)\n",
    "\n",
    "        # 运行核函数\n",
    "        reduction_kernel_simple[grid](\n",
    "            vector_fp16, output_fp16, vec_size, BLOCK_SIZE=block_size\n",
    "        )\n",
    "\n",
    "        reduction_kernel_simple[grid](\n",
    "            vector_fp32, output_fp32, vec_size, BLOCK_SIZE=block_size\n",
    "        )\n",
    "\n",
    "        # 计算最终结果\n",
    "        result_fp16 = output_fp16.sum().item()\n",
    "        result_fp32 = output_fp32.sum().item()\n",
    "\n",
    "        results_fp16.append(result_fp16)\n",
    "        results_fp32.append(result_fp32)\n",
    "\n",
    "        print(f\"\\n线程块大小 {block_size}:\")\n",
    "        print(f\"  float16结果: {result_fp16:.10f}\")\n",
    "        print(f\"  与PyTorch差异: {abs(result_fp16 - torch_sum_fp16):.10f}\")\n",
    "        print(f\"  float32结果: {result_fp32:.10f}\")\n",
    "        print(f\"  与PyTorch差异: {abs(result_fp32 - torch_sum_fp32):.10f}\")\n",
    "\n",
    "    # 分析结果\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"结果分析:\")\n",
    "    max_diff_fp16 = max(results_fp16) - min(results_fp16)\n",
    "    max_diff_fp32 = max(results_fp32) - min(results_fp32)\n",
    "\n",
    "    print(f\"float16不同线程块间的最大差异: {max_diff_fp16:.10f}\")\n",
    "    print(f\"float32不同线程块间的最大差异: {max_diff_fp32:.10f}\")\n",
    "\n",
    "    if max_diff_fp32 > 0:\n",
    "        print(f\"float16差异是float32的 {max_diff_fp16 / max_diff_fp32:.2f} 倍\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 检查Triton是否可用\n",
    "    try:\n",
    "        import triton\n",
    "\n",
    "        print(f\"Triton版本: {triton.__version__}\")\n",
    "        # 运行演示\n",
    "        demonstrate_nondeterministic_triton()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"错误: Triton未安装\")\n",
    "        print(\"请使用以下命令安装: pip install triton\")\n",
    "    except Exception as e:\n",
    "        print(f\"运行出错: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "    print(\"\\n演示完成!\")\n"
   ],
   "id": "e4d02ce4790be8ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.1 输出\n",
    "\n",
    "运行信息：\n",
    "\n",
    "* GPU：NVIDIA A100-SXM4-80GB\n",
    "* 固件：Driver Version: 570.172.08     CUDA Version: 12.8\n",
    "* 镜像：nvcr.io/nvidia/pytorch:24.09-py3\n",
    "\n",
    "```\n",
    "Triton版本: 3.3.0\n",
    "================================================================================\n",
    "Triton非确定性计算演示\n",
    "使用不同线程块大小和内存访问模式\n",
    "================================================================================\n",
    "使用设备: cuda\n",
    "\n",
    "1. 测试不同线程块大小的向量归约\n",
    "----------------------------------------\n",
    "向量大小: 1,048,576\n",
    "数据范围: [-4.594, 4.797]\n",
    "\n",
    "PyTorch sum (float16): -143.8750000000\n",
    "PyTorch sum (float32): -143.8523254395\n",
    "两者差异: 0.0226745605\n",
    "\n",
    "线程块大小 64:\n",
    "  float16结果: -143.8328857422\n",
    "  与PyTorch差异: 0.0421142578\n",
    "  float32结果: -143.8524475098\n",
    "  与PyTorch差异: 0.0001220703\n",
    "\n",
    "线程块大小 128:\n",
    "  float16结果: -143.6943359375\n",
    "  与PyTorch差异: 0.1806640625\n",
    "  float32结果: -143.8524169922\n",
    "  与PyTorch差异: 0.0000915527\n",
    "\n",
    "线程块大小 256:\n",
    "  float16结果: -144.1240234375\n",
    "  与PyTorch差异: 0.2490234375\n",
    "  float32结果: -143.8523254395\n",
    "  与PyTorch差异: 0.0000000000\n",
    "\n",
    "线程块大小 512:\n",
    "  float16结果: -144.4716796875\n",
    "  与PyTorch差异: 0.5966796875\n",
    "  float32结果: -143.8524780273\n",
    "  与PyTorch差异: 0.0001525879\n",
    "\n",
    "线程块大小 1024:\n",
    "  float16结果: -144.1484375000\n",
    "  与PyTorch差异: 0.2734375000\n",
    "  float32结果: -143.8524169922\n",
    "  与PyTorch差异: 0.0000915527\n",
    "\n",
    "----------------------------------------\n",
    "结果分析:\n",
    "float16不同线程块间的最大差异: 0.7773437500\n",
    "float32不同线程块间的最大差异: 0.0001525879\n",
    "float16差异是float32的 5094.40 倍\n",
    "\n",
    "演示完成!\n",
    "```\n"
   ],
   "id": "644230d6567a2086"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 附：矩阵乘法的比较",
   "id": "9bfacc28d1da9975"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@triton.jit\n",
    "def matrix_mult_kernel_simple(\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,\n",
    "        stride_bk, stride_bn,\n",
    "        stride_cm, stride_cn,\n",
    "        BLOCK_SIZE_M: tl.constexpr,\n",
    "        BLOCK_SIZE_N: tl.constexpr,\n",
    "        BLOCK_SIZE_K: tl.constexpr,\n",
    "):\n",
    "    \"\"\"简单的矩阵乘法核函数\"\"\"\n",
    "    pid_m = tl.program_id(axis=0)\n",
    "    pid_n = tl.program_id(axis=1)\n",
    "\n",
    "    # 计算偏移\n",
    "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "\n",
    "    # 计算指针\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # 累加器\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "\n",
    "    # 矩阵乘法\n",
    "    for k in range(0, K, BLOCK_SIZE_K):\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n",
    "        accumulator += tl.dot(a, b)\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    # 存储结果\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, accumulator, mask=mask)"
   ],
   "id": "149cc389de0670e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def matrix_compare():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"使用设备: {device}\")\n",
    "\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # 禁用确定性算法\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "\n",
    "    # 创建矩阵数据\n",
    "    mat_size = 256  # 减小矩阵大小以加快计算\n",
    "    A_fp16 = torch.randn((mat_size, mat_size), dtype=torch.float16, device=device)\n",
    "    B_fp16 = torch.randn((mat_size, mat_size), dtype=torch.float16, device=device)\n",
    "    A_fp32 = A_fp16.float()\n",
    "    B_fp32 = B_fp16.float()\n",
    "\n",
    "    print(f\"矩阵大小: {mat_size}x{mat_size}\")\n",
    "\n",
    "    # PyTorch基准\n",
    "    torch_result_fp16 = torch.matmul(A_fp16, B_fp16)\n",
    "    torch_result_fp32 = torch.matmul(A_fp32, B_fp32)\n",
    "\n",
    "    target_element = (50, 50)  # 选择一个目标元素\n",
    "    torch_val_fp16 = torch_result_fp16[target_element].item()\n",
    "    torch_val_fp32 = torch_result_fp32[target_element].item()\n",
    "\n",
    "    print(f\"PyTorch结果 (float16): {torch_val_fp16:.10f}\")\n",
    "    print(f\"PyTorch结果 (float32): {torch_val_fp32:.10f}\")\n",
    "\n",
    "    # 不同图块大小的Triton矩阵乘法\n",
    "    tile_configs = [\n",
    "        {\"BLOCK_SIZE_M\": 32, \"BLOCK_SIZE_N\": 32, \"BLOCK_SIZE_K\": 32},\n",
    "        {\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32},\n",
    "        {\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 128, \"BLOCK_SIZE_K\": 32},\n",
    "    ]\n",
    "\n",
    "    matmul_results_fp16 = []\n",
    "    matmul_results_fp32 = []\n",
    "\n",
    "    for config in tile_configs:\n",
    "        # 准备输出张量\n",
    "        C_fp16 = torch.empty((mat_size, mat_size), dtype=torch.float16, device=device)\n",
    "        C_fp32 = torch.empty((mat_size, mat_size), dtype=torch.float32, device=device)\n",
    "\n",
    "        # 计算网格大小\n",
    "        grid_m = triton.cdiv(mat_size, config[\"BLOCK_SIZE_M\"])\n",
    "        grid_n = triton.cdiv(mat_size, config[\"BLOCK_SIZE_N\"])\n",
    "        grid = (grid_m, grid_n)\n",
    "\n",
    "        # 运行核函数\n",
    "        matrix_mult_kernel_simple[grid](\n",
    "            A_fp16, B_fp16, C_fp16,\n",
    "            mat_size, mat_size, mat_size,\n",
    "            A_fp16.stride(0), A_fp16.stride(1),\n",
    "            B_fp16.stride(0), B_fp16.stride(1),\n",
    "            C_fp16.stride(0), C_fp16.stride(1),\n",
    "            BLOCK_SIZE_M=config[\"BLOCK_SIZE_M\"],\n",
    "            BLOCK_SIZE_N=config[\"BLOCK_SIZE_N\"],\n",
    "            BLOCK_SIZE_K=config[\"BLOCK_SIZE_K\"],\n",
    "        )\n",
    "\n",
    "        matrix_mult_kernel_simple[grid](\n",
    "            A_fp32, B_fp32, C_fp32,\n",
    "            mat_size, mat_size, mat_size,\n",
    "            A_fp32.stride(0), A_fp32.stride(1),\n",
    "            B_fp32.stride(0), B_fp32.stride(1),\n",
    "            C_fp32.stride(0), C_fp32.stride(1),\n",
    "            BLOCK_SIZE_M=config[\"BLOCK_SIZE_M\"],\n",
    "            BLOCK_SIZE_N=config[\"BLOCK_SIZE_N\"],\n",
    "            BLOCK_SIZE_K=config[\"BLOCK_SIZE_K\"],\n",
    "        )\n",
    "\n",
    "        # 获取结果\n",
    "        val_fp16 = C_fp16[target_element].item()\n",
    "        val_fp32 = C_fp32[target_element].item()\n",
    "\n",
    "        matmul_results_fp16.append(val_fp16)\n",
    "        matmul_results_fp32.append(val_fp32)\n",
    "\n",
    "        print(f\"\\n图块配置 {config}:\")\n",
    "        print(f\"  float16: {val_fp16:.10f}, 差异: {abs(val_fp16 - torch_val_fp16):.10f}\")\n",
    "        print(f\"  float32: {val_fp32:.10f}, 差异: {abs(val_fp32 - torch_val_fp32):.10f}\")\n",
    "\n",
    "\n",
    "matrix_compare()"
   ],
   "id": "b5093c246c47112b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
