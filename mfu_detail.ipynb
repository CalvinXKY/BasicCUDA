{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN6c97vOXN7pkANCBi7h6m9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CalvinXKY/BasicCUDA/blob/master/mfu_detail.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "主要介绍LLM（GPT/Llama/MoE）中一些操作层、模块的flops计算量，帮助理解MFU计算过程。\n",
        "\n",
        "![transformer架构](./images/transformer.png)\n",
        "\n",
        "# 基本模块flops计算\n",
        "## 线性层的计算量\n",
        "\n",
        "线性层的计算公式为 Y = wX + b 涉及到矩阵的乘法与加法运算。\n",
        "\n",
        "矩阵乘法与加法的flops的计算为：\n",
        "\n",
        "**乘法计算量**：对于两个矩阵A和B的乘法C=AB，其中A是m×n矩阵，B是n×p矩阵，C是m×p矩阵。每个元素Cij需要进行 n 次乘法和n-1次加法，总共有mp个元素，因此总FLOPS为：\n",
        "\n",
        "mp(n+(n-1)) = 2mnp - mp。\n",
        "\n",
        "**加法/减法计算量**：对于两个矩阵A和B的加法C=A+B，其中A和B都是m×n矩阵，C也是m×n矩阵。每个元素Cij需要进行一次加法，总共有mn个元素，因此总FLOPS为mn。\n",
        "\n",
        "对于linear计算，里面涉及一个矩阵乘和一个矩阵加法，由于元素需要展平再运算，权重w的维度[m, n] 输入的维度是[1, n] 输出维度[1, m]，其计算量为\n",
        "\n",
        "2mn\n",
        "\n",
        "不考虑bias的计算量为\n",
        "\n",
        "2mn  - m\n",
        "\n",
        "对于transformer的线性层输入与输出一般用相同的大小，形状都为：[batch_size, seq_len, d_model],\n",
        "线性层的创建一般使用 Linear(hidden_size, hidden_size, bias=False)\n",
        "所以计算量为：\n",
        "\n",
        "flops = 2 * batch_size * seq_len * hidden_size * hidden_size\n",
        "\n",
        "如果不一致时：\n",
        "flops = 2 * batch_size * seq_len * size_1 * size_2"
      ],
      "metadata": {
        "id": "odaFtoySwx8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWjh_Y9fwgfY"
      },
      "outputs": [],
      "source": [
        "def calcu_linear_flops(batch_size, seq_len, hidden_size, head=0, d_model=0,bias=False):\n",
        "    bias_flops = 0 if not bias else batch_size * seq_len * hidden_size\n",
        "    if head ==0:\n",
        "        flops = 2 * batch_size * seq_len * hidden_size * hidden_size + bias_flops\n",
        "    else:\n",
        "        flops = 2 * batch_size * seq_len * hidden_size * head * d_model + bias_flops\n",
        "    return flops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention模块的计算\n",
        "\n",
        "一般的MHA(MultiHeadAttention)计算的构造如下：\n",
        "\n",
        "```\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(input_dim, output_dim)\n",
        "        self.key = nn.Linear(input_dim, output_dim)\n",
        "        self.value = nn.Linear(input_dim, output_dim)\n",
        "        self.dk = output_dim\n",
        "\n",
        "    # Scaled dot-product attention:\n",
        "    def self_attention(self, query, key, value, mask):\n",
        "        # query/key/value:  (bs,  seq_len, dk)/(bs, heads, seq_len, dk)\n",
        "        # mask shape = (bs, 1, seq_len)/(bs, 1, 1, seq_len)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.dk)) # (bs, seq_len, seq_len)/(bs, heads, seq_len, seq_len)\n",
        "        if mask is not None:\n",
        "            scores.masked_fill_(mask == torch.tensor(False), float(\"-inf\"))\n",
        "        # Softmax dim=-1 stands for apply the softmax along the last dimension\n",
        "        attention_weights = nn.Softmax(dim=-1)(scores)  # (bs, heads, seq_len, seq_len)/(bs, seq_len, seq_len)\n",
        "        attention_qkv = torch.matmul(attention_weights, value)   # (bs, seq_len, dk)/(bs, heads, seq_len, dk)\n",
        "        return attention_qkv\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        # qkv shape: (bs, seq_len, d_model)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)\n",
        "        value = self.value(value)\n",
        "        attention_qkv = self.self_attention(query, key, value, mask)  # shape:  (bs, seq_len, d_model)\n",
        "        return attention_qkv\n",
        "\n",
        "class MultiHeadedAttention(Attention):\n",
        "    def __init__(self, d_model, heads):\n",
        "        super().__init__(d_model, d_model)\n",
        "        assert d_model % heads == 0\n",
        "        self.dk = d_model // heads  # head dimension\n",
        "        self.heads = heads\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "        self.sqrt_dk = torch.sqrt(torch.tensor(self.dk))\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        batch_size = query.shape[0]\n",
        "        # qkv shape: (bs, seq_len, dk*heads)\n",
        "        # dk * heads = d_model\n",
        "        query = self.query(query).view(batch_size, -1, self.heads, self.dk).transpose(1, 2)\n",
        "        key = self.key(key).view(batch_size, -1, self.heads, self.dk).transpose(1, 2)\n",
        "        value = self.value(value).view(batch_size, -1, self.heads, self.dk).transpose(1, 2)\n",
        "        attention_qkv = self.self_attention(query, key, value, mask)  # shape:  (bs, heads, seq_len, dk)\n",
        "        #  (bs, heads, seq_len, dk) -> (bs, seq_len, dk*heads)\n",
        "        reshaped = attention_qkv.transpose(1, 2).reshape(batch_size, -1, self.heads * self.dk)\n",
        "        representations_batch = self.out_linear(reshaped)\n",
        "        return representations_batch\n",
        "```\n",
        "\n",
        "主要运算：\n",
        "* Q/K/V: 线性映射\n",
        "* scores: QK乘法运算\n",
        "* attention_qkv: V和attention_weights乘法运算\n",
        "* out_linear: 线性度计算\n",
        "\n",
        "次要运算：\n",
        "* softmax计算\n",
        "* masked_fill计算\n",
        "\n",
        "对于主要运算中有个需要考虑点：\n",
        "* Attention的变化：query_attention的计算KV的heads数量与Q的heads数量不一致。\n",
        "* 序列并行（context parallel/ring attention）: 考虑并行度。\n",
        "\n",
        "次要运算在估算flops时通常可以忽略，这里例出其计算方式：\n",
        "\n",
        "softmax的flops计算量： 输入的shape：(bs, heads, seq_len, seq_len)\n",
        "元素计算涉及指数运算、加法运算、除法运算。计算量：\n",
        "\n",
        "   3 * bs * heads * seq_len * (seq_len - 1)\n",
        "\n",
        "maked_fill是一个掩模操作包含：判断操作和赋值操作，假设是需要遍历整个矩阵，每个元素操作一次，而赋值操作仅对需要操作的元素赋值，输入矩阵的大小为[bs, heads, seq_len, seq_len], 操作的个数为X。所以计算量：\n",
        "\n",
        "   bs * heads *  seq_len * seq_len + X\n",
        "\n",
        "由于X操作相对来说较小, 公式简化为：\n",
        "\n",
        "   bs * heads *  seq_len * seq_len"
      ],
      "metadata": {
        "id": "zzJMXD-s7MFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_attention_flops(batch_size, seq_len, heads, d_model, num_query_groups=0, context_parallel=1):\n",
        "    num_query_groups = num_query_groups if num_query_groups != 0 else heads\n",
        "    q_linear_flops = calcu_linear_flops(batch_size, seq_len, heads * d_model)\n",
        "    k_linear_flops = calcu_linear_flops(batch_size, seq_len, heads * d_model, num_query_groups, d_model)\n",
        "    v_linear_flops = k_linear_flops\n",
        "\n",
        "    kv_scores_flops = 2 * batch_size * seq_len**2 * heads * d_model * (context_parallel + 1) / (2 * context_parallel)\n",
        "    mask_flops = batch_size * heads *  seq_len * seq_len\n",
        "    softmax_flops = 3 * batch_size * heads * seq_len * (seq_len - 1)\n",
        "\n",
        "    qkv_flops = kv_scores_flops\n",
        "    out_linear_flops = calcu_linear_flops(batch_size, seq_len, heads * d_model)\n",
        "    return q_linear_flops + k_linear_flops + v_linear_flops + kv_scores_flops + mask_flops + softmax_flops + qkv_flops + out_linear_flops"
      ],
      "metadata": {
        "id": "5azr-HUe7K_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LayerNorm/RMSNorm\n",
        "\n",
        "Layer_norm的计算内容一般如下：\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "\n",
        "def layer_normalization(x, epsilon=1e-8):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True) # 最后一个维度\n",
        "    std = np.std(x, axis=-1, keepdims=True)\n",
        "    normalized_x = (x - mean) / (std + epsilon)\n",
        "    return normalized_x\n",
        "\n",
        "```\n",
        "假设数据的长度为L\n",
        "包含平均值计算、标准差计算、偏移计算；\n",
        "* mean计算包含L加法和一次除法：  L + 1\n",
        "* std计算，每个元素进行一个减法、一个乘法、一个加法。最后进行一个除法和一个乘法操作： 3*L + 2\n",
        "* 标准化：每个元素一次减法、一次除法操作： 2*L\n",
        "\n",
        "忽略单次运算，所以操作计算量：\n",
        "\n",
        "6 * batch_size * seq_len * hidden_size"
      ],
      "metadata": {
        "id": "wTw0eoK49BQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_layer_norm_flops(batch_size, seq_len, hidden_size):\n",
        "  return 6 * batch_size * seq_len * hidden_size"
      ],
      "metadata": {
        "id": "m-FXTKEsUk-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSNorm 常见的代码实现如下：\n",
        "\n",
        "```\n",
        "# 参考Llama定义\n",
        "class LlamaRMSNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-6):\n",
        "        \"\"\"\n",
        "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        input_dtype = hidden_states.dtype\n",
        "        hidden_states = hidden_states.to(torch.float32)\n",
        "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
        "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
        "        return self.weight * hidden_states.to(input_dtype)\n",
        "```\n",
        "\n",
        "主要计算内容：\n",
        "* 元素二次方、元素求平均（n-1）、一个rsqrt运算、一个求和运算\n",
        "* 两个乘法操作\n",
        "\n",
        "忽略单次运算，flops数等于：\n",
        "\n",
        "  4 * batch_size * seq_len * hidden_size\n"
      ],
      "metadata": {
        "id": "f7jPf4ankDIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_rmsnorm_flops(batch_size, seq_len, hidden_size):\n",
        "  return 4 * batch_size * seq_len * hidden_size"
      ],
      "metadata": {
        "id": "oq9splUCpQ7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP/FFN层的计算\n",
        "\n",
        "MLP层的构建常见的方式如下：\n",
        "\n",
        "```\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, dff=2048):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, dff)\n",
        "        self.linear2 = nn.Linear(dff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, representations_batch):\n",
        "        return self.linear2(self.relu(self.linear1(representations_batch)))\n",
        "```\n",
        "\n",
        "主要包含两个线性层操作和一个Relu计算。\n",
        "\n",
        "输入/输出: [batch_size, seq_len, hidden_size]\n",
        "dff值：ffn_hidden_size\n",
        "\n",
        "计算量为两次线性运算 + 一个relu操作，其flops操作数量如下：\n",
        "2 * batch_size * seq_len * hidden_size * ffn_hidden_size + batch_size * seq_len * ffn_hidden_size\n",
        "\n",
        "Llama的MLP有些改动，一般的计算包含三次线性运算（gate_proj、up_proj、down_proj, 参看hugging face的LlamaMLP定义）一个silu运算，一个元素乘法运算。\n",
        "\n",
        "[LlamaMLP定义](https://github.com/huggingface/transformers/blob/v4.48.0/src/transformers/models/llama/modeling_llama.py#L174):\n",
        "\n",
        "![llama_mlp](./images/llama_mlp.png)\n",
        "\n",
        "```\n",
        "# L174\n",
        "class LlamaMLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
        "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
        "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
        "        self.act_fn = ACT2FN[config.hidden_act]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "        return down_proj\n",
        "```\n",
        "\n",
        "对应的flops计算工作：\n",
        "\n",
        "3 * batch_size * seq_len * hidden_size * ffn_hidden_size + 2 * batch_size * seq_len * ffn_hidden_size\n",
        "\n"
      ],
      "metadata": {
        "id": "qGcCHN9FWAqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_mlp_flops(batch_size, seq_len, hidden_size, ffn_hidden_size, use_gate=True):\n",
        "  \"\"\"\n",
        "   use_gate=True SwiGLU structure FFN.\n",
        "  \"\"\"\n",
        "  if use_gate:\n",
        "    flops = 3 * 2 * batch_size * seq_len * hidden_size * ffn_hidden_size + 2 * batch_size * seq_len * ffn_hidden_size\n",
        "  else:\n",
        "    flops = 2 * 2 * batch_size * seq_len * hidden_size * ffn_hidden_size + batch_size * seq_len * ffn_hidden_size\n",
        "  return flops"
      ],
      "metadata": {
        "id": "xuKwm6uRWZI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logits计算\n",
        "\n",
        "logits计算包含三个运算：\n",
        "\n",
        "* layernorm\n",
        "* linaer，(词表映射)\n",
        "* softmax\n",
        "\n",
        "对应尺寸\n",
        "* layernorm/rmsnorm: [batch_size, seq_len, hidden_size]\n",
        "* linear: input:[batch_size，seq_len*hidden_size] output: :[batch_size，seq_len*vocab_size]\n",
        "* softmax: [batch_size，seq_len*vocab_size]\n",
        "\n",
        "对应计算量：\n",
        "\n",
        "6 * batch_size * seq_len * hidden_size\n",
        "\n",
        "batch_size * seq_len * hidden_size * vocab_size\n",
        "\n",
        "3 * batch_size * seq_len * (vocab_size - 1)"
      ],
      "metadata": {
        "id": "t6DlpZmofGTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_logits_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size, RMSNorm=True):\n",
        "    norm_flops = calcu_rmsnorm_flops(batch_size, seq_len, hidden_size) if RMSNorm else \\\n",
        "    calcu_layer_norm_flops(batch_size, seq_len, hidden_size)\n",
        "\n",
        "    linear_flops = 2 * batch_size * seq_len * hidden_size * vocab_size\n",
        "\n",
        "    softmax_flos = 3 * batch_size * seq_len * (vocab_size - 1)\n",
        "    return norm_flops + linear_flops + softmax_flos"
      ],
      "metadata": {
        "id": "k5JJMN9RMi3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 位置编码计算\n",
        "\n",
        "Transformer采用的位置编码PE：包含正弦/余弦运算，对每个位置进行d_model/2正弦,d_model/2余弦，计算量为：\n",
        "\n",
        "seq_len * d_model\n",
        "\n",
        "注：如果进行了多头切分， d_model = d_model * heads\n",
        "\n",
        "如果采用旋转位置编码RoPE：\n",
        "\n",
        "* 旋转角度计算：d_model\n",
        "* 每个位置计算构造旋转矩阵：seq_len * d_model\n",
        "* Q，K与旋转矩阵乘法：4 * batch_size * seq_len * d_model"
      ],
      "metadata": {
        "id": "IHGpT-_Mtee2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_position_encoding(batch_size, seq_len, heads, d_model, pe_type=\"rope\"):\n",
        "  if pe_type == \"rope\":\n",
        "    return 4 * batch_size * seq_len * d_model * heads\n",
        "  else:\n",
        "    return seq_len * d_model * heads"
      ],
      "metadata": {
        "id": "T_BV5Tuu6dHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Router计算\n",
        "\n",
        "router计算主要是在MoE中应用，其计算一般包括：\n",
        "\n",
        "```\n",
        "self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n",
        "hidden_states = hidden_states.view(-1, hidden_dim)\n",
        "# router_logits: (batch * sequence_length, n_experts)\n",
        "router_logits = self.gate(hidden_states)\n",
        "\n",
        "routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
        "routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
        "```\n",
        "主要是一个gate线性层计算：\n",
        "\n",
        "flops = 2 * batch_size * seq_len * hidden_size * num_experts\n"
      ],
      "metadata": {
        "id": "SKnpS9ZhJPhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_router_flops(batch_size, seq_len, hidden_size, experts):\n",
        "  return 2 * batch_size * seq_len * hidden_size * experts"
      ],
      "metadata": {
        "id": "4xvLDG7Uk0L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 典型训练模型的flops计算\n",
        "\n",
        "训练估算约定:\n",
        "\n",
        "1、模型backward计算是forward计算大约两倍, 因为需要计算输入 + 权重的梯度 [参考](https://arxiv.org/pdf/2205.05198)。\n",
        "\n",
        "2、输入的Embedding层主要完成映射计算, 输入维度[batch_size, seq_len] 输出维度： (bs, seq_len, d_model)，其flops计算量可以忽略。 其权重用于LM-head计算时对应的计算量在logits中考虑。\n",
        "\n",
        "![LM_Head_Weights](./images/lm_head_weights.png)\n",
        "\n",
        "3、位置编码的计算量相对较小，给与忽略。\n",
        "\n",
        "\n",
        "## GPT结构flops计算\n",
        "\n",
        "模型涉及计算的主要结构\n",
        "\n",
        "decoder_layer x N + logtis\n",
        "\n",
        "其中L是层数，decoder构成：\n",
        "\n",
        "MHA + FFN + 2 LayerNorm\n",
        "\n",
        "![GPT架构](./images/gpt_architecture.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "8fQ7-5fyNC-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def caclu_gpt_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size, ffn_hidden_size, layer_nums):\n",
        "  attention_flops = calcu_attention_flops(batch_size, seq_len, heads, d_model, num_query_groups=0, context_parallel=1)\n",
        "  ffn_flops = calcu_mlp_flops(batch_size, seq_len, hidden_size, ffn_hidden_size, use_gate=False)\n",
        "  layer_norm_flops = calcu_layer_norm_flops(batch_size, seq_len, hidden_size)\n",
        "  logits_flops = calcu_logits_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size, False)\n",
        "  return 3 * (logits_flops + (layer_norm_flops * 2 + attention_flops + ffn_flops) * layer_nums)"
      ],
      "metadata": {
        "id": "w71MVb2hHifU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLAMA结构flops计算\n",
        "\n",
        "结构：\n",
        "\n",
        "(GMQA + FFN + RMSNorm) x L + logtis\n",
        "\n",
        "其中GMQA 是group attention， FFN： Feed ForwardSwiGLU结构。\n",
        "\n",
        "![Llama架构](./images/llama_architecture.png)"
      ],
      "metadata": {
        "id": "BkKN5SASMSqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def caclu_llama_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size, ffn_hidden_size, layer_nums, num_query_groups):\n",
        "  attention_flops = calcu_attention_flops(batch_size, seq_len, heads, d_model, num_query_groups=num_query_groups, context_parallel=1)\n",
        "  ffn_flops = calcu_mlp_flops(batch_size, seq_len, hidden_size, ffn_hidden_size, use_gate=True)\n",
        "  layer_norm_flops = calcu_layer_norm_flops(batch_size, seq_len, hidden_size)\n",
        "  logits_flops = calcu_logits_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size)\n",
        "  return 3 * (logits_flops + (layer_norm_flops * 2 + attention_flops + ffn_flops) * layer_nums)"
      ],
      "metadata": {
        "id": "ssOaoymHN6Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MoE模型flops计算\n",
        "\n",
        "在llama结构基础上ffn增加topk专家数量系数，计算公式：\n",
        "\n",
        "(GMQA + FFN * Experts_topk + Router + RMSNorm) x L + logtis\n",
        "\n"
      ],
      "metadata": {
        "id": "8jvl60XLLo9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def caclu_moe_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size, ffn_hidden_size, layer_nums, num_query_groups, topk, experts):\n",
        "  attention_flops = calcu_attention_flops(batch_size, seq_len, heads, d_model, num_query_groups=num_query_groups, context_parallel=1)\n",
        "  ffn_flops = calcu_mlp_flops(batch_size, seq_len, hidden_size, ffn_hidden_size, use_gate=True)\n",
        "  layer_norm_flops = calcu_layer_norm_flops(batch_size, seq_len, hidden_size)\n",
        "  logits_flops = calcu_logits_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size)\n",
        "  router_flops = calcu_router_flops(batch_size, seq_len, hidden_size, experts)\n",
        "  return 3 * (logits_flops + (layer_norm_flops * 2 + attention_flops + ffn_flops * topk + router_flops) * layer_nums)"
      ],
      "metadata": {
        "id": "Xis5osj3PURF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MoE如果包括了共享专家(shared experts)，上述计算公式中将topk的数量设置为：\n",
        "\n",
        "topk + shared_experts_nums\n",
        "\n"
      ],
      "metadata": {
        "id": "oX3KT77g0bOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MFU计算\n",
        "\n",
        "MFU(Model Flops Utilization)计算的公式为：\n",
        "\n",
        "MFU = 单位时间实际flops/单位时间名义flops\n",
        "\n",
        "单位时间实际flops = 单步模型计算flops总数/单步迭代时间\n",
        "\n",
        "MFU = model_flops_sum / iter_time * (device_peak_flops * device_num)\n",
        "\n",
        "通常device_peak_flops的单位为： TFlops/s"
      ],
      "metadata": {
        "id": "HdfKN3xcPtR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcu_moe_mfu(iter_time, batch_size, seq_len, heads, d_model, hidden_size, vocab_size, ffn_hidden_size, layer_nums, num_query_groups, topk, experts, device_nums, device_peak_flops):\n",
        "    model_flops = caclu_moe_flops(batch_size, seq_len, heads, d_model, hidden_size, vocab_size, ffn_hidden_size, layer_nums, num_query_groups, topk, experts)\n",
        "    return model_flops / (iter_time * device_peak_flops * device_nums * 10 ** 12)\n"
      ],
      "metadata": {
        "id": "9MSWETZ2RUmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试："
      ],
      "metadata": {
        "id": "iCFd5ZZyStTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calcu_moe_mfu(iter_time=1.5,\n",
        "       batch_size=1024, seq_len=4096, heads=8, d_model=128,\n",
        "       hidden_size=1024, vocab_size=32768, ffn_hidden_size=2048,\n",
        "       layer_nums=100, num_query_groups=4, topk=9, experts=100,\n",
        "       device_nums=1024, device_peak_flops=280)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hGazgz3Sx8A",
        "outputId": "cab05084-e7f8-4932-c0fa-a3a2e2580133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4019112012361143"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 参考内容：\n",
        "\n",
        "https://zhuanlan.zhihu.com/p/691126108\n",
        "\n",
        "https://github.com/naklecha/llama3-from-scratch/blob/main/README.md\n",
        "\n",
        "https://arxiv.org/pdf/2205.05198\n",
        "\n",
        "https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/te_llama/tutorial_accelerate_hf_llama_with_te.html\n",
        "\n",
        "https://github.com/huggingface/transformers/blob/v4.48.0/src/transformers/models/qwen2_moe/modeling_qwen2_moe.py#L603\n",
        "\n",
        "https://bbycroft.net/llm"
      ],
      "metadata": {
        "id": "DFnkp-9VWZcF"
      }
    }
  ]
}